{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd049a24-70aa-4394-ba15-b13f337d2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Offense Case Type Offense Date Offense Time Violation Charge Code  \\\n",
      "0                NT    10/1/2023      0:26:00                 60110   \n",
      "1                TR    10/1/2023     13:40:00                   710   \n",
      "2                TR    10/1/2023     16:39:00                 32210   \n",
      "3                TR    10/1/2023     17:04:00                   610   \n",
      "4                OR    10/1/2023     10:26:00                 64111   \n",
      "\n",
      "                Offense Charge Description       Offense Street Name  \\\n",
      "0                      Public Intoxication            4905 TERI ROAD   \n",
      "1                            Ran Red Light       4010 SOUTHWEST PKWY   \n",
      "2  Crossing Property To Turn Right Or Left         2414 S LAMAR BLVD   \n",
      "3                            Ran Stop Sign          BLUE CREST DRIVE   \n",
      "4                        Animal - At Large  6600 BLOCK ASHLAND DRIVE   \n",
      "\n",
      "  Offense Cross Street  School Zone  Construction Zone Case Closed   Race  \\\n",
      "0                  NaN        False              False        TERM  White   \n",
      "1                  NaN        False              False        TERM  White   \n",
      "2                  NaN        False              False        TERM  White   \n",
      "3      GOODRICH AVENUE        False              False        TERM  White   \n",
      "4                  NaN        False              False        TERM  White   \n",
      "\n",
      "  Defendant Gender                    Agency Officer Code  \n",
      "0             Male  Austin Police Department         9711  \n",
      "1           Female  Austin Police Department         9441  \n",
      "2             Male  Austin Police Department         7238  \n",
      "3             Male  Austin Police Department         7238  \n",
      "4           Female  Austin Police Department         7078  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(r'C:\\courses\\DataManagement and Vis\\project\\Municipal_Court_Caseload_Information_FY_2023.csv')\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e15a4-9c9e-415c-97de-acfa469f8cb2",
   "metadata": {},
   "source": [
    "Counting Total Rows and Unique Street names in Main dataset Municipal_Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f00741b-34b6-4bdc-92aa-d50f0c24df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             STREET  POSTCODE    CITY\n",
      "0  DOYLE OVERTON RD     78719  Austin\n",
      "1           MAHA RD     78719  Austin\n",
      "2       POCMONT TRL     78719  Austin\n",
      "3         EVELYN RD     78747  Austin\n",
      "4    S SH  45 E  WB     78747  Austin\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(r'C:\\courses\\DataManagement and Vis\\project\\austin_street_to_zip_mapping.csv')\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ac2e6-7ed3-416f-b105-6f2481afdb35",
   "metadata": {},
   "source": [
    "##Cleaning Street columns in main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d1c02910-51fd-4597-a7d0-3946b489cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacements =  {\n",
    "    \"ROAD\": \"RD\", \"RD\": \"RD\",\n",
    "    \"STREET\": \"ST\", \"ST\": \"ST\",\n",
    "    \"AVENUE\": \"AVE\", \"AVE\": \"AVE\",\n",
    "    \"DRIVE\": \"DR\", \"DR\": \"DR\",\n",
    "    \"COURT\": \"CT\", \"CT\": \"CT\",\n",
    "    \"LANE\": \"LN\", \"LN\": \"LN\",\n",
    "    \"CIRCLE\": \"CIR\", \"CIR\": \"CIR\",\n",
    "    \"TRAIL\": \"TRL\", \"TRL\": \"TRL\",\n",
    "    \"PARKWAY\": \"PKWY\", \"PKWY\": \"PKWY\",\n",
    "    \"HIGHWAY\": \"HWY\", \"HWY\": \"HWY\",\n",
    "    \"PLACE\": \"PL\", \"PL\": \"PL\",\n",
    "    \"PATH\": \"PATH\",\n",
    "    \"WAY\": \"WAY\",\n",
    "    \"LOOP\": \"LOOP\",\n",
    "    \"COVE\": \"CV\", \"CV\": \"CV\",\n",
    "\n",
    "    # directions (important!)\n",
    "    \"NORTH\": \"N\", \"N\": \"N\",\n",
    "    \"SOUTH\": \"S\", \"S\": \"S\",\n",
    "    \"EAST\": \"E\", \"E\": \"E\",\n",
    "    \"WEST\": \"W\", \"W\": \"W\",\n",
    "    \"NORTHEAST\": \"NE\", \"NE\": \"NE\",\n",
    "    \"NORTHWEST\": \"NW\", \"NW\": \"NW\",\n",
    "    \"SOUTHEAST\": \"SE\", \"SE\": \"SE\",\n",
    "    \"SOUTHWEST\": \"SW\", \"SW\": \"SW\",\n",
    "\n",
    "    # lane directions\n",
    "    \"NB\": \"NB\", \"SB\": \"SB\", \"EB\": \"EB\", \"WB\": \"WB\"\n",
    "}\n",
    "\n",
    "direction_garbage = {\n",
    "   \"BLOCK\", \"NB\", \"SB\", \"EB\", \"WB\", \"NORTHBOUND\", \"SOUTHBOUND\", \"EASTBOUND\", \"WESTBOUND\"\n",
    "}\n",
    "ordinals = {\n",
    "    \"FIRST\": \"1ST\", \"SECOND\": \"2ND\", \"THIRD\": \"3RD\", \"FOURTH\": \"4TH\",\n",
    "    \"FIFTH\": \"5TH\", \"SIXTH\": \"6TH\", \"SEVENTH\": \"7TH\", \"EIGHTH\": \"8TH\",\n",
    "    \"NINTH\": \"9TH\", \"TENTH\": \"10TH\"\n",
    "}\n",
    "\n",
    "def clean_and_normalize(s):\n",
    "    if pd.isna(s) or s == 'nan':\n",
    "        return s\n",
    "\n",
    "    s = s.upper()\n",
    "\n",
    "    # deleting numbers in first position with a space \n",
    "    s = re.sub(r\"^\\d+\\s+\", \"\", s)\n",
    "\n",
    "    # deleting punctuations\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "\n",
    "    tokens = s.split()\n",
    "    clean_tokens = []\n",
    "\n",
    "    for i, t in enumerate(tokens):\n",
    "        # deleting noises (NB, SB, etc)\n",
    "        if t in direction_garbage:\n",
    "            continue\n",
    "        #FIRST -> 1ST\n",
    "        t = ordinals.get(t, t)\n",
    "\n",
    "        # correctint the abbrevations (ROAD -> RD)\n",
    "        t = replacements.get(t, t)\n",
    "\n",
    "        clean_tokens.append(t)\n",
    "\n",
    "    return \" \".join(clean_tokens).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4443e-9ae2-46e3-b760-84f12ffc85be",
   "metadata": {},
   "source": [
    "##Clean and normalize main dataset's street name Municipal_Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdbad3ef-8aa7-43e7-a607-21cd1ff999ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in first dataset Municipal_Court: 225624\n",
      "Number of rows after deleting duplicated names: 5449\n"
     ]
    }
   ],
   "source": [
    "df1[\"city_clean\"] = df1[\"Offense Street Name\"].apply(clean_and_normalize)\n",
    "\n",
    "count_before1 = len(df1)\n",
    "#removing duplicated streets\n",
    "df1_unique = df1.drop_duplicates(subset=[\"city_clean\"])\n",
    "count_after1 = len(df1_unique)\n",
    "\n",
    "print(f\"Total number of rows in first dataset Municipal_Court: {count_before1}\")\n",
    "print(f\"Number of rows after deleting duplicated names: {count_after1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db6dc97f-604b-48b6-8c0f-612c8d9e1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_unique.to_csv(\"df1_unique_streets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b54c3208-0837-4008-b0c0-90a0e780898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                TERI RD\n",
      "1                SW PKWY\n",
      "2           S LAMAR BLVD\n",
      "3          BLUE CREST DR\n",
      "4             ASHLAND DR\n",
      "5            W PARMER LN\n",
      "6          LEVANDER LOOP\n",
      "7          LEVANDER LOOP\n",
      "8                LAZY ON\n",
      "9       WALSH TARLTON LN\n",
      "10          S MOPAC EXPY\n",
      "11    S IH 35 SERVICE RD\n",
      "12    S IH 35 SERVICE RD\n",
      "13    S IH 35 SERVICE RD\n",
      "14      WALSH TARLTON LN\n",
      "15      WALSH TARLTON LN\n",
      "16            RIO GRANDE\n",
      "17                W 22ND\n",
      "18                W 22ND\n",
      "19           SAN GABRIEL\n",
      "Name: city_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df1[\"city_clean\"].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf9631-3f9c-4a8b-b45b-fe798254107d",
   "metadata": {},
   "source": [
    "##Clean and normalize second dataset's street name austin_street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72e32ea6-0712-4272-b382-2ca0912ca4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with an empty string, then clean\n",
    "df2[\"city_clean\"] = df2[\"STREET\"].fillna(\"\").astype(str).str.upper().str.strip()\n",
    "\n",
    "# Your normalization function now receives \"\" instead of a float\n",
    "df2[\"city_clean\"] = df2[\"city_clean\"].apply(clean_and_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ed4a13bd-55dd-4a01-836e-cd438777e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in second dataset austin_street: 11840\n",
      "Number of rows after deleting duplicated names: 11773\n"
     ]
    }
   ],
   "source": [
    "count_before2 = len(df2)\n",
    "\n",
    "#removing duplicated streets\n",
    "df2_unique = df2.drop_duplicates(subset=[\"city_clean\", \"POSTCODE\"]) # there are some street name with diffretnt zipcode, so they are not equal\n",
    "\n",
    "count_after2 = len(df2_unique)\n",
    "\n",
    "print(f\"Total number of rows in second dataset austin_street: {count_before2}\")\n",
    "print(f\"Number of rows after deleting duplicated names: {count_after2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a496879e-eaa1-4822-b421-d78d4c0e1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_unique.to_csv(\"df2_unique_streets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1d8629b3-ba2b-4c55-a853-e3f74caf647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    DOYLE OVERTON RD\n",
      "1             MAHA RD\n",
      "2         POCMONT TRL\n",
      "3           EVELYN RD\n",
      "4           S SH 45 E\n",
      "Name: city_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"city_clean\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be553f-53eb-467b-874d-ceda59f2fcc5",
   "metadata": {},
   "source": [
    "##to find how many unique cities matches in 2 different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad878dd4-e91d-4dc4-b706-d88cc27e3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 1986\n"
     ]
    }
   ],
   "source": [
    "#Integerating 2 datasets\n",
    "common = set(df1_unique[\"city_clean\"]) & set(df2_unique[\"city_clean\"])\n",
    "# Remove 'nan' or empty strings if they aren't real cities -> Data Quality\n",
    "common.discard('NAN') \n",
    "common.discard('')\n",
    "common.discard('nan')\n",
    "print(\"Matches:\", len(common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9ee0a010-c046-4546-bc41-a48fbcecd57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLAR DR', 'W 22ND ST', 'MC CURDY ST', 'ASHEN LN', 'PINE KNOLL DR', 'LAWRENCE ST', 'W WELLS BRANCH PKWY', 'CHIPPEWAY LN', 'MEADOW CREEK DR', 'DOYAL DR']\n"
     ]
    }
   ],
   "source": [
    "common_list = list(common)\n",
    "\n",
    "print(common_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56cf8805-9fbb-4d14-a499-0613f2f990fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S LAMAR BLVD exists in both files!\n"
     ]
    }
   ],
   "source": [
    "search_name = \"S LAMAR BLVD\" \n",
    "\n",
    "if search_name in common:\n",
    "    print(f\"✅ {search_name} exists in both files!\")\n",
    "else:\n",
    "    print(f\"❌ {search_name} was not found in the common list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db629fc3-1eea-4358-b3b9-ed5140f29d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E 51ST CLARKSON AVE', 'BURTON RD', 'RED BUD TRL', 'E PARMER PN', 'FAIR FIELD DR', 'MILTON E', 'S 1 ST ST', 'S SH 130 SERVICE RD', 'MOPAC LOT', 'N MOPAC SVRD']\n"
     ]
    }
   ],
   "source": [
    "missing_streets = set(df1[\"city_clean\"]) - set(df2[\"city_clean\"])\n",
    "print(list(missing_streets)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6bf81821-7d1c-4006-b139-737a718f0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common ZIPs: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{78701,\n",
       " 78702,\n",
       " 78703,\n",
       " 78704,\n",
       " 78705,\n",
       " 78721,\n",
       " 78722,\n",
       " 78723,\n",
       " 78724,\n",
       " 78725,\n",
       " 78726,\n",
       " 78727,\n",
       " 78728,\n",
       " 78729,\n",
       " 78730,\n",
       " 78731,\n",
       " 78732,\n",
       " 78733,\n",
       " 78735,\n",
       " 78736,\n",
       " 78737,\n",
       " 78738,\n",
       " 78739,\n",
       " 78741,\n",
       " 78742,\n",
       " 78744,\n",
       " 78745,\n",
       " 78746,\n",
       " 78747,\n",
       " 78748,\n",
       " 78749,\n",
       " 78750,\n",
       " 78751,\n",
       " 78752,\n",
       " 78753,\n",
       " 78754,\n",
       " 78756,\n",
       " 78757,\n",
       " 78758,\n",
       " 78759}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neigh = pd.read_csv(r'C:\\courses\\DataManagement and Vis\\project\\austin_zip_to_neighborhood_full.csv')\n",
    "df_pop = pd.read_csv(r'C:\\courses\\DataManagement and Vis\\project\\austin_population_by_zip_scraped.csv')\n",
    "valid_pop = df_pop[df_pop[\"Population\"] > 0]\n",
    "\n",
    "zip_neigh = set(df_neigh[\"ZIP_Code\"])\n",
    "zip_pop_valid = set(valid_pop[\"ZIP_Code\"])\n",
    "zip_street = set(df2[\"POSTCODE\"])\n",
    "\n",
    "common_zips = zip_neigh & zip_pop_valid & zip_street\n",
    "\n",
    "print(\"Common ZIPs:\", len(common_zips))\n",
    "common_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "09fd10b0-5248-46ca-ad26-9cd85d38797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common streets with valid ZIP: 1703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['OLD LOCKHART RD', 'THAXTON RD', 'BRADSHAW RD', 'PINEHURST DR',\n",
       "       'OLD SAN ANTONIO RD', 'EDEN DR', 'MARY LEWIS DR', 'SUNDAY DR',\n",
       "       'ONION CREEK PKWY', 'SOUTHERNER WAY', 'DIMITRIOS DR',\n",
       "       'PRESTON TRAILS DR', 'BELL TOWER LN', 'DEER CHASE TRL',\n",
       "       'CROWN COLONY DR', 'KENNEDY ST', 'ABBY ANN LN', 'BOCA RATON DR',\n",
       "       'STEINBECK DR', 'FARRAH LN', 'PINNACLE CREST LOOP', 'MANCHACA RD',\n",
       "       'BRODIE LN', 'INTERLACHEN LN', 'BUZZ SCHNEIDER LN', 'SUNSET DR',\n",
       "       'PAVELICH PASS', 'HACIENDA DR', 'DAVE SILK DR', 'WINTER HAVEN RD'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_zip_common = df2[df2[\"STREET\"].isin(common)\n",
    "]\n",
    "\n",
    "street_zip_common_valid = street_zip_common[\n",
    "    street_zip_common[\"POSTCODE\"].isin(common_zips)\n",
    "]\n",
    "streets_with_valid_zip = street_zip_common_valid[\"STREET\"].unique()\n",
    "\n",
    "print(\"Number of common streets with valid ZIP:\", len(streets_with_valid_zip))\n",
    "streets_with_valid_zip[:30]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
